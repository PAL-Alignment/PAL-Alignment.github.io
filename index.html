<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A framework to model heterogeneous human preference with multiple reward models">
  <meta name="keywords" content="AI alignment, Heterogeneous Preference Learning, Large Language Models, Reward Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</title>

  <!-- MathJax -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N6HD47TZ5G"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-N6HD47TZ5G');
  </script>

  <link rel="icon" href="./static/images/icon.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    hr.custom-line {
        border: none;
        border-top: 2px solid black;
        width: 100%;
        margin: 20px auto;
    }
    hr.custom-line-dashed {
        border: none;
        border-top: 2px dashed black;
        width: 100%;
        margin: 20px auto;
    }
  </style>

</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- title -->
          <h1 class="title is-1 publication-title"><text style="color:#ff3860;">PAL</text>: <text style="color:#ff3860;">P</text><text class="is-size-2">luralistic</text> <text style="color:#ff3860;">AL</text><text class="is-size-2">ignment </text><text class="is-size-2">Framework</text></h1>
          <h3 class="title is-2 publication-title is-size-3">for Learning from Heterogeneous Preferences</h3>
          <br>
          <h5 class="subtitle is-5 publication-awards">ICML TF2M/MFHAIA workshop 2024 (Oral)</h5>
          <!-- authors info -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chendaiwei-99.github.io/" target="_blank">Daiwei Chen</a>,</span>
            <span class="author-block">
              <a href="https://www.deepneural.network/" target="_blank">Yi Chen</a>,</span>
            <span class="author-block">
              <a href="https://aniketrege.github.io/" target="_blank">Aniket Rege</a>,
            </span>
            <span class="author-block">
              <a href="https://ramyakv.github.io/" target="_blank">Ramya Korlakai Vinayak</a>
            </span>
          </div>
          <!-- authors institute -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>University of Wisconsin-Madison</b></span>
          </div>
          <!-- links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.08469"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.08469"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/ChenDaiwei-99/PAL-Pluralistic-Alignment-Framework"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ramya-ml/pickapic-embeds"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h4 class="subtitle has-text-centered">
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>PAL</b> has been accepted at 2024 ICML workshops:
        <a href="https://sites.google.com/view/tf2m" target="_blank">TF2M</a> and <a href="https://sites.google.com/view/mhf-icml2024" target="_blank">MFHAIA</a>.
      </h4>
      <img src="./static/images/PAL-teaser.png" alt="Description of the image">
      <p>
        PAL is a framework designed to learn reward models for alignment that suit diverse, heterogeneous human preferences. For some user \(i\), the probability of 
        preferring the left item \(x_l\) to right item \(x_r\) given conditioning \(x_c\) is given by reward model \(r_\theta\). PAL learns \(r_\theta\) with a mixture
        modeling approach over \(K\) prototypes that represent \(K\) subgroups - this captures the shared structure of human preferences (subpopulations), while also 
        suiting the specific differences between individual people.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences.
            Such alignment is typically done by collecting large amounts of pairwise comparisons from humans ("Do you prefer output A or B?") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences.
            These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences.
          </p> -->
          <p>
            We propose PAL, an alignment framework complementary to existing pretraining strategies, which is designed to model the diverse, heterogeneous nature (<em><a href="https://arxiv.org/abs/2402.05070">plurality</a></em>) of human preferences from the ground up.
            Contrary to the status-quo paired preference <a href="https://www.jstor.org/stable/2334029">Bradley-Terry model</a> used by many popular LLMs and LVMs today, we reframe the alignment problem using <a href="https://psycnet.apa.org/record/1951-00045-001">Coombs' ideal point model</a> and a mixture modeling approach. 
            PAL captures the plurality of the preferences of a population of users while simultaneously learning a shared preference latent space, which can few-shot generalize to new, unseen users.
            PAL demonstrates the efficacy of cheap and efficient reward modeling - our approach learns reward functions via simple MLP layers (on top of the penultimate-layer dense representations learned by large pretrained foundation models) that are on-par with the existing large state-of-the-art reward models which are typically in the billion parameter regime. 
            We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with OpenAI's <a href="https://github.com/openai/summarize-from-feedback">TL;DR dataset</a> ; 2) Text-to-Image models with the <a href="https://stability.ai/research/pick-a-pic">Pick-a-Pic dataset</a> ; 3) A new semi-synthetic heterogeneous dataset generated using <a href="https://www.evals.anthropic.com/">Anthropic Personas</a>. 
          </p>
          <p>
            Our experiments also highlight a key shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and we make a case for more nuanced data collection approaches.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <hr class="custom-line">
    <div class="columns is-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3 has-text-centered"> <img src="./static/images/icon-yes-no.png" style="width: 5%; height: auto;">&nbsp; What is the Ideal Point Model ?</h2>
        <img src="./static/images/ideal-point-model.png" alt="Description of the image">
        <p>
          The ideal point model (<a href="https://psycnet.apa.org/record/1951-00045-001">Coombs, 1950</a>) is a statistical model that is used to analyze the preferences of individuals or groups.
          It is broadly used in political science, sociology, and economics to model the preferences of voters, legislators, or other decision-makers.
          The ideal point model assumes that <strong style="color: #ff3860;">each individual has an "ideal point"</strong> in some high-dimensional space \(\mathbb{R}^d\), and that the individual's preference for a particular alternative 
          is a function of <b>the distance between the alternative and the individual's ideal point</b>. The ideal point model is well suited for heterogeneous preference learning, which we discuss below.
        </p>
      </div>
    </div>
    <!-- <hr class="custom-line"> -->
    <div class="columns is-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3 has-text-centered"><img src="./static/images/icon-user-group.png" style="width: 5%; height: auto;">&nbsp; PAL: Mixture Modeling for Heterogeneous Preferences</h2>
        <img src="./static/images/PAL-models.png" alt="Description of the image">
        <p>
          In reality, different people can have different preferences that are not just noisy perturbations of a universal model, i.e. <strong style="color: #ff3860;">people can differ in systematically different ways</strong>! 
          People's preferences are not completely unique - there are shared aspects of preferences within <em>subgroups</em> of people, for example owing to similar demographics and educational, socio-cultural, or other types of similarities. 
          PAL is designed to suit this structure of human preferences - in particular, we use <b>a mixture modeling approach</b> to capture diverse individual preferences across <b>\(K\) subgroups</b>, where each user's preference (ideal point) is a convex combination of:
        </p>
        <p>
          <ul>
            <li><b><br>&nbsp;Model A</b>: \(K\) prototypical ideal points.</li>
            <li><b>&nbsp;Model B</b>: \(K\) protypical functions mapping input prompts to ideal points.</li>
          </ul>
        </p>
        <p>
          <br> Here the \(K\) prototypes represent the shared structure across subpopulations, while each users' weights \(W\) over the prototypes represent their individuality.
        </p>
      </div>
    </div>
    <hr class="custom-line">
    <h2 class="title is-3 has-text-centered"><img src="./static/images/icon-gaussian.png" style="width: 5%; height: auto;">&nbsp; Experiments: Gaussian Synthetic Dataset</h2>
    <h3 class="title is-5 ">Hypothesis: if we create a synthetic dataset with a "ground truth" subpopulation structure (injecting heterogeneity), PAL should be able to learn these groups well.</h3>
    <div class="columns is-centered">
      <div class="column is-six-fifths">
        <p>
          Assume \( K^* \) "true" user prototypes \(\{\mathbf{p}_i\}_{i=1}^{K^*}\), where \(\mathbf{p}_i \sim \mathcal{N}(0,(1/d)I)\). We consider two settings: 
        </p>
        <br>
        <p>
          <ul>
            <li>&nbsp;&nbsp; 1) <strong>Mixture</strong> setting: each user is located in the convex hull of \( K \) learned prototypes</li>
            <li>&nbsp;&nbsp; 2) <strong>Partition</strong> setting: \( N \) users are evenly sampled from \( K \) learned prototypes, with \(\mathbf{a}_i \in \{\mathbf{p}_k\}_{k=1}^{K}\)</li>
          </ul>
        </p>
        <br>
        <p>
          Each sample is generated as follows: we randomly draw two items \(\{\mathbf{x}_l, \mathbf{x}_r\}\) and one user \(\mathbf{a}_i\), and label the user's preference as \(\text{sign}(\|f^*(\mathbf{x}_l)-f^*(\mathbf{a}_i)\|_2-\|f^*(\mathbf{x}_r)-f^*(\mathbf{a}_i)\|_2)\). 
          We generate a total of \( n \) samples per user to learn the user's ideal point. We use model A with a single-layer MLP (without bias) with hinge loss and evaluate on the held-out test set, which we split into two disjoint sets:
          <ul>
            <li><br>&nbsp;&nbsp; 1) <strong>Seen user</strong>: user who provides labeled preferences in train and test set.</li>
            <li>&nbsp;&nbsp; 2) <strong>Unseen user</strong>: user who provides labeled preferences in test set but not train set.</li>
          </ul>
        </p>
        <br>
        <h4 class="title is-5 ">Results:</h4>
        <img src="./static/images/gaussian-performance.png">
        <p>
          <ul>
            <li>&nbsp;(a) <b>Learnability</b>: PAL can align the <strong style="color: #ff8838;">learned</strong> and <strong style="color: #383bff;">true</strong> user ideal points in the representation space \(\mathbb{R}^3\).</li>
            <li>&nbsp;(b) <b>Adapting to Plurality</b>: a PAL <strong style="color: #f7af79;">homogeneous</strong> reward model (\( K = 1 \)) is suboptimal when <strong style="color: #6605c1;">diverse preferences</strong> exist (\( K^* \gt 1 \)). 
            When we allow for learning plurality by setting \( K \gt 1 \), PAL enables a significant 5-7% accuracy gain. </li>
            <li>&nbsp;(c) <b>Sample Complexity</b>: as we increase # training samples for seen users, PAL achieves higher test accuracy, and is also more accurate in capturing # true prototypes in the dataset (accuracy peaks at \( K = K^* \)).</li>
            <li>&nbsp;(d) <b>Generalization</b>: for unseen users (users not in the train set), PAL can generalize to accurately predict preferences with \( \sim 50 \) labeled examples.</li>
          </ul>
        </p>
      </div>
    </div>
    <hr class="custom-line">
    <h2 class="title is-3 has-text-centered"><img src="./static/images/icon-dataset.png" style="width: 5%; height: auto;">&nbsp; Experiments: Heterogeneous Semi-Synthetic Datasets</h2>
    <h3 class="title is-5 ">Hypothesis: if we synthetically inject diverse preferences into real datasets, PAL should still be able to learn these groups reasonably well.</h3>
    <div class="columns is-centered">
      <div class="column is-half">
        <h5 class="title is-5 has-text-centered"><em>Persona</em> <text style="font-size: small;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/persona.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a href="https://www.evals.anthropic.com/">Anthropic Personas</a> dataset contains a collection of personalities or personas, each associated with 500 statements that align with the persona and 500 statements that do not.
          <br> <br>
          We create a heterogeneous preference dataset by sampling pairs of persona statements and imitating the preference choices of subpopulation groups with diverse personality preferences.
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Results</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
            <img src="./static/images/persona-performance.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            (a) On the heterogeneous persona dataset, we observe that as # learned prototypes approach the # true prototypes, i.e. <strong style="color: #ff3860;">\(K \to K^\star\), the seen accuracy increases to \(100\%\)</strong> 
            given a sufficient number of users and number of comparisons per user ; (b) As we get more comparisons per seen user \(n_p\), PAL eventually saturates to 100% accuracy ( when \(K\geq K^\star=3\))
          </p>
        </p>
      </div>
      <div class="column is-half">
        <h5 class="title is-5 has-text-centered"><em>Pick-a-Filter</em> <text style="font-size: small;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/pick-a-filter.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a href="https://stability.ai/research/pick-a-pic">Pick-a-Pic dataset</a> is a large, crowdsourced open dataset of human preferences over text-to-image generation, designed to align pre-trained models with human preferences.
          <br> <br>
          We construct the Pick-a-Filter dataset by assuming two subpopulation groups that prefer warm (red) or cool (blue) tones. We apply simple color filters to Pick-a-Pic V1 to semi-synthetically inject this heterogeneity.
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Results</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
            <img src="./static/images/pick-a-filter-performance.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            <!-- We train our proposed reward model, PAL Model-B (PAL-B), on the Pick-a-Filter dataset with different mixture ratio and number of prototypes.
            PAL-B effectively captures diverse preferences across mixture ratios in Pick-a-Filter. -->
            <!-- <br><br> -->
            PAL enables learning beyond a universal preference \(K^* > 1\) to identify diverse user preference groups. 
            We observe that <strong style="color: #ff3860;">PAL significantly outperforms the homogeneous reward model</strong> in predicting user preferences - at a mixture ratio of 1, PAL achieves <strong>\(95.2\%\) </strong> test accuracy compared to \(75.4\%\) from the homogeneous reward model (\(K=1\)).
          </p>
        </p>
      </div>
    </div>
    <hr class="custom-line">
  </div>
</section>









<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024pal,
        title={PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences},
        author={Chen, Daiwei and Chen, Yi and Rege, Aniket and Vinayak, Ramya Korlakai},
        journal={arXiv preprint arXiv:2406.08469},
        year={2024}
      }</code></pre>
  </div>
</section>


<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
      <p>
        This work was supported by <b>NSF grants NCS-FO 2219903</b> and <b>NSF CAREER Award CCF 2238876</b>.
      </p>
      <p>
        <b>Usage and License Notice</b>: The data, code and model checkpoints are intended for research use.
      </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="font-size: 12px;">
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
