<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A framework to model heterogeneous human preference with multiple reward models">
  <meta name="keywords" content="AI alignment, Heterogeneous Preference Learning, Large Language Models, Reward Model, Pluralistic Alignment">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences</title>

  <!-- MathJax -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N6HD47TZ5G"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-N6HD47TZ5G');
  </script>

  <link rel="icon" href="./static/images/icon.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    hr.custom-line {
        border: none;
        border-top: 2px solid black;
        width: 100%;
        margin: 20px auto;
    }
    hr.custom-line-dashed {
        border: none;
        border-top: 2px dashed black;
        width: 100%;
        margin: 20px auto;
    }
  </style>

</head>

<body>

<section class="hero">
  <div class="hero-body" style="padding-bottom:0em">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- title -->
          <h1 class="title is-1 publication-title"><text style="color:#ff3860;">PAL</text>: <text style="color:#ff3860;">P</text><text class="is-size-2">luralistic</text> <text style="color:#ff3860;">AL</text><text class="is-size-2">ignment </text><text class="is-size-2">Framework</text></h1>
          <h3 class="title is-2 publication-title is-size-3">for Learning from Heterogeneous Preferences</h3>
          <br>
          <!-- <h5 class="subtitle is-5 publication-awards">NIPS AFM workshop 2024</h5> -->
          <!-- <h5 class="subtitle is-5 publication-awards">NIPS Behavior ML workshop 2024</h5> -->
          <!-- <h5 class="subtitle is-5 publication-awards">NIPS FITML workshop 2024</h5> -->
          <!-- <h5 class="subtitle is-5 publication-awards">NIPS Pluralistic Alignment workshop 2024</h5> -->
          <!-- <h5 class="subtitle is-5 publication-awards">NIPS SoLaR workshop 2024</h5> -->
          <h5 class="subtitle is-5 publication-awards">ðŸ”¥<span style="color: #ff3860">[NEW!] </span>NeurIPS <a href="https://adaptive-foundation-models.org/" target="_blank">AFM</a> / <a href="https://sites.google.com/view/behavioralml/" target="_blank">Behavioral ML</a> / <a href="https://sites.google.com/view/neurips2024-ftw/home" target="_blank">FITML</a> / <a href="https://pluralistic-alignment.github.io/" target="_blank">Pluralistic Alignment</a> / <a href="https://solar-neurips.github.io/" target="_blank">SoLaR</a> workshop 2024 (Spotlight)</h5>
          <h5 class="subtitle is-5 publication-awards">ICML <a href="https://sites.google.com/view/tf2m" target="_blank">TF2M</a> / <a href="https://sites.google.com/view/mhf-icml2024" target="_blank">MFHAIA</a> workshop 2024 (Oral)</h5>
          <!-- authors info -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chendaiwei-99.github.io/" target="_blank">Daiwei Chen</a>,</span>
            <span class="author-block">
              <a href="https://www.deepneural.network/" target="_blank">Yi Chen</a>,</span>
            <span class="author-block">
              <a href="https://aniketrege.github.io/" target="_blank">Aniket Rege</a>,
            </span>
            <span class="author-block">
              <a href="https://zwang.org/" target="_blank">Zhi Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://ramyakv.github.io/" target="_blank">Ramya Korakai Vinayak</a>
            </span>
          </div>
          <!-- authors institute -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>University of Wisconsin-Madison</b></span>
          </div>
          <!-- links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.08469"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.08469"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/ChenDaiwei-99/PAL-Pluralistic-Alignment-Framework"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ramya-ml/pickapic-embeds"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank"
                   rel="noopener noreferrer">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div>
      <!-- <h4 class="subtitle has-text-centered">
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>PAL Framework</b> has been accepted by 2024 NeurIPS workshop <br>
        (<a href="https://adaptive-foundation-models.org/" target="_blank">AFM</a>, <a href="https://sites.google.com/view/behavioralml/" target="_blank">Behavior ML</a>, <a href="https://sites.google.com/view/neurips2024-ftw/home" target="_blank">FITML</a>, <a href="https://pluralistic-alignment.github.io/" target="_blank">Pluralistic Alignment</a>, <a href="https://solar-neurips.github.io/" target="_blank">SoLaR</a>)
      </h4>
      <h4 class="subtitle has-text-centered">
        </span> <b>PAL Framework</b> has been accepted by 2024 ICML workshop 
        (<a href="https://sites.google.com/view/tf2m" target="_blank">TF2M</a> and <a href="https://sites.google.com/view/mhf-icml2024" target="_blank">MFHAIA</a>)
      </h4> -->
      <img src="./static/images/PAL-teaser-color.png" alt="Description of the image">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences.
            Such alignment is typically done by collecting large amounts of pairwise comparisons from humans ("Do you prefer output A or B?") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences.
            These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences.
          </p> -->
          <p style="margin-bottom: 1em;">
            Foundation models trained on internet-scale data benefit from extensive alignment to human preferences before deployment. 
            However, existing methods typically assume a <strong style="font-style: italic; font-family:Georgia; color: black">homogeneous preference</strong> shared by all individuals, overlooking the diversity inherent in human values. 
            In this work, we propose a general reward modeling framework for <strong style="font-style: italic; font-family:Georgia; color:black">pluralistic alignment</strong> </strong> <strong style="font-family:'Courier New', Courier, monospace; font-size:large">(PAL)</strong>, which incorporates diverse preferences from the ground up.
            <br><br>
            This approach not only addresses the limitations of traditional alignment methods but also offers several key advantages that make it particularly efficient for handling diverse human preferences:
          </p>
          <!-- <hr class="custom-line"> -->
          <hr style="margin: 0.1em;">
          <p style="margin-bottom: 0em;">
            <strong class="is-size-5" style="color: #007BFF; text-align: left;">Diverse Preference Alignment:</strong> PAL can handle diverse human preferences rather than assuming a single, universal preference, addressing the variability in individual values. 
          </p>
          <hr style="margin: 0.1em;">
          <p style="margin-bottom: 0em;">
            <strong class="is-size-5" style="color: #28A745; text-align: left;">Higher Performance with Fewer Parameters:</strong> e.g. for a T2T task, PAL is 1.7% more accurate for seen users and 36% more accurate for unseen users, with 20Ã— fewer parameters. 
          </p>
          <hr style="margin: 0.1em;">
          <p style="margin-bottom: 0em;">
            <strong class="is-size-5" style="color: #FFC107; text-align: left;">Modular Design:</strong> PAL's architecture is modular, allowing it to leverage shared common preferences while adapting to specific individual preferences. 
          </p>
          <hr style="margin: 0.1em;">
          <p style="margin-bottom: 0em;">
            <strong class="is-size-5" style="color: #DC3545; text-align: left;">Few-shot Generalization:</strong> PAL enables quick adaptation to new users' preferences with few examples, making it more efficient for personalized alignment. 
          </p>
          <hr style="margin: 0.1em;">
          <div style="border: 2px; margin: 5px;">
            <img src="./static/images/pal-reddit-main-result.png" alt="Description of the image" style="width: 70%; height: auto; display: block; margin: auto;">
            <br>
            <p style="width: 100%; display: block; margin: auto; font-size:large">
              <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> is <strong style="font-style: italic; font-family:Georgia; color: black">accuracy-compute optimal</strong> and shows <strong style="font-style: italic; font-family:Georgia; color: black"> state-of-the-art (SoTA) performance</strong>.
            </p>
          </div>
          <!-- <p>
            In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up.
            We propose using the ideal point model as a lens to view alignment using preference comparisons.
            Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users.
            Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. 
            We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. 
          </p>
          <p>
            Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <hr class="custom-line">
    <div class="columns is-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3 has-text-centered"> <img src="./static/images/icon-yes-no.png" style="width: 5%; height: auto;">&nbsp; What is Ideal Point Model ?</h2>
        <img src="./static/images/IPM.png" alt="Description of the image">
        <p>
          The ideal point model (<a href="https://psycnet.apa.org/record/1951-00045-001">Coombs, 1950</a>) is a statistical model that is used to analyze the preferences of individuals or groups.
          It is broadly used in political science, sociology, and economics to model the preferences of voters, legislators, or other decision-makers.
          The ideal point model assumes that <strong style="color: #ff3860;">each individual has an "ideal point"</strong> in some high-dimensional space \(\mathbb{R}^d\), and that the individual's preference for a particular alternative 
          is a function of <b>the distance between the alternative and the individual's ideal point</b>. The ideal point model is well suited for heterogeneous preference learning, which we discuss below.
        </p>
      </div>
    </div>
    <hr class="custom-line">
    <div class="columns is-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3 has-text-centered"><img src="./static/images/icon-user-group.png" style="width: 5%; height: auto;">&nbsp; PAL: Mixture Modeling for Heterogeneous Preferences</h2>
        <img src="./static/images/PAL-models.png" alt="Description of the image">
        <p>
          In reality, different people can have different preferences that are not just noisy perturbations of a universal model, i.e. <strong style="color: #ff3860;">people can differ in systematically different ways</strong>! 
          People's preferences are not completely unique - there are shared aspects of preferences within <em>subgroups</em> of people, for example owing to similar demographics and educational, socio-cultural, or other types of similarities. 
          PAL is designed to suit this structure of human preferences - in particular, we use <b>a mixture modeling approach</b> to capture diverse individual preferences across <b>\(K\) subgroups</b>, where each user's preference (ideal point) is a convex combination of:
        </p>
        <p>
          <ul>
            <li><b><br>&nbsp;Model A</b>: \(K\) prototypical ideal points.</li>
            <li><b>&nbsp;Model B</b>: \(K\) protypical functions mapping input prompts to ideal points.</li>
          </ul>
        </p>
        <p>
          <br> Here the \(K\) prototypes represent the shared structure across subpopulations, while each users' weights \(W\) over the prototypes represent their individuality.
        </p>
      </div>
    </div>
    <hr class="custom-line">
    <h2 class="title is-3 has-text-centered"><img src="./static/images/icon-dataset.png" style="width: 5%; height: auto;">&nbsp; Experiments on Heterogeneous Preference Datasets</h2>
    <div class="columns is-centered">
      <div class="column is-half" style="box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); border: 2px dashed; margin: 5px;">
        <h5 class="title is-5 has-text-centered"><em>Reddit TL;DR Summary </em> <text style="font-size: medium;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/reddit-dataset.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a href="https://arxiv.org/pdf/2009.01325">TL;DR Summary dataset</a> contains a series of preferences over summaries generated by language models. 
          For each pair of summaries, a labeler determines which one is preferred or not. 
          <br> <br>
          We used the variant of the TL;DR dataset proposed by <a href="https://arxiv.org/abs/2402.05133">Li et al., 2024</a>, which uses the summary length as the preference. 
          The majority group prefers longer summaries while the minority prefers shorter summaries. 
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Performance</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
            <img src="./static/images/performance-reddit.png" alt="Experiment Results" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            The table reports the performance of <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL-B-Large</strong> (OPT-350M) compared to SoTA <a href="https://arxiv.org/abs/2402.05133">P-DPO</a>. 
            We observe that <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong>, <strong style="color: #ff3860;">with around 6.3 billion fewer parameters</strong>, is <strong style="color: #ff3860;">\(1.7\%\)</strong> more accurate on seen users, and <strong style="color: #ff3860;">\(36\%\)</strong> more accurate on unseen users.
            <br><br>
            The figure illustrates <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong>'s ability to generalize effectively to unseen users in few-shot settings. 
            As the number of samples per unseen user increases, <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> progressively adapts to their preferences. 
            With just <strong style="color: #ff3860;">\(20\) samples per unseen user</strong>, <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> achieves performance comparable to that of seen users.
          </p>
        </p>
      </div>
      <div class="column is-half"  style="box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); border: 2px dashed; margin: 5px;">
        <h5 class="title is-5 has-text-centered"><em>Pick-A-Pic</em> <text style="font-size: medium;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/pick-a-pic.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
          &nbsp;
          <img src="./static/images/pick-a-pic-2.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a>Pick-a-Pic dataset</a> is a large, open dataset for human feedback in T2I generation. 
          There are two versions of Pick-a-Pic, v1 and v2, where v2 extends v1.
          <br> <br>
          To ensure a fair model evaluation, we divide the v2 test set into <em>no-leakage</em> and <em>leakage</em> subsets due to overlap (<em>leakage</em>) with the v1 train set. 
          We only consider the 18391 samples with no preference ties, i.e. one generated image is always preferred to the other. 
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Performance</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
            <img src="./static/images/performance-pick-a-pic.png" alt="Experiment Results" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            The table shows <strong>(a)</strong> <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> matches SoTA PickScore when trained on V1 <strong style="color: #ff3860;">with 165\(\times\) fewer parameters</strong>; 
            <strong>(b)</strong> <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> exceeds SoTA performance on v2-no-leakage (i.e. fair comparison) by <strong style="color: #ff3860;">\(2\%\)</strong> when training on v1, and by <strong style="color: #ff3860;">\(2.5\%\)</strong> if training on v2. 
            <!-- Training <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> from scratch i.e. with CLIP-H embeddings, outperforms training on PickScore embeddings (which were trained on v1).  -->
             <br><br>
            We note that <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL-B-Tiny</strong> (\(\sim\)6M params) exceeds SoTA performance while training on <strong style="color: #ff3860;">a single RTX 4090 GPU</strong>, whereas PickScore (\(\sim\)1B params) is trained with 8\(\times\)A100 GPUs -- highlighting the suitability of <strong style="font-family:'Courier New', Courier, monospace; font-size:large">PAL</strong> for efficient and democratic reward modeling.
          </p>
        </p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-half" style="box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); border: 2px dashed; margin: 5px;">
        <h5 class="title is-5 has-text-centered"><em>Persona</em> <text style="font-size: medium;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/persona.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a href="https://www.evals.anthropic.com/">Anthropic Personas</a> dataset contains a collection of personalities or personas, each associated with 500 statements that align with the persona and 500 statements that do not.
          <br> <br>
          We create a heterogeneous preference dataset by sampling pairs of persona statements and imitating the preference choices of subpopulation groups with diverse personality preferences.
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Results</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
            <img src="./static/images/persona-performance.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            (a) On the heterogeneous persona dataset, we observe that as # learned prototypes approach the # true prototypes, i.e. <strong style="color: #ff3860;">\(K \to K^\star\), the seen accuracy increases to \(100\%\)</strong> 
            given a sufficient number of users and number of comparisons per user ; (b) As we get more comparisons per seen user \(n_p\), PAL eventually saturates to 100% accuracy ( when \(K\geq K^\star=3\))
          </p>
        </p>
      </div>
      <div class="column is-half" style="box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); border: 2px dashed; margin: 5px;">
        <h5 class="title is-5 has-text-centered"><em>Pick-a-Filter</em> <text style="font-size: medium;">dataset</text></h5>
        <div class="is-flex is-align-items-center is-justify-content-center" style="height: 200px;">
          <img src="./static/images/pick-a-filter.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
        </div>
        <p>
          The <a href="https://stability.ai/research/pick-a-pic">Pick-a-Pic dataset</a> is a large, crowdsourced open dataset of human preferences over text-to-image generation, designed to align pre-trained models with human preferences.
          <br> <br>
          We construct the Pick-a-Filter dataset by assuming two subpopulation groups that prefer warm (red) or cool (blue) tones. We apply simple color filters to Pick-a-Pic V1 to semi-synthetically inject this heterogeneity.
        </p>
        <hr class="custom-line-dashed">
        <p>
          <img src="./static/images/icon-performance.png" style="width: 4%; height: auto;">&nbsp; <strong>Performance</strong>: 
          <div class="is-flex is-align-items-center is-justify-content-center" style="height: 250px;">
            <img src="./static/images/performance-pick-a-filter.png" alt="Description of the image" style="max-height: 100%; max-width: 100%;">
          </div>
          <p>
            <!-- We train our proposed reward model, PAL Model-B (PAL-B), on the Pick-a-Filter dataset with different mixture ratio and number of prototypes.
            PAL-B effectively captures diverse preferences across mixture ratios in Pick-a-Filter. -->
            <!-- <br><br> -->
            PAL enables learning beyond a universal preference \(K^* > 1\) to identify diverse user preference groups. 
            We observe that <strong style="color: #ff3860;">PAL significantly outperforms the homogeneous reward model</strong> in predicting user preferences - at a mixture ratio of 1, PAL achieves <strong>\(95.2\%\) </strong> test accuracy compared to \(75.4\%\) from the homogeneous reward model (\(K=1\)).
          </p>
        </p>
      </div>
    </div>

    <hr class="custom-line">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 0px"><img src="./static/images/icon-theory.png" style="width: 5%; height: auto">&nbsp; Theoretical Guarantees and Sample Complexity</h2>
    <div class="column is-centered has-text-centered">
      <div class="column is-six-fifths" style="display:flex; align-items: center; flex-direction: column">
        <div style="display: inline-block; border: 2px solid blue; border-radius: 8px; text-align: center; margin: 20px; padding: 6px; width: 80%; box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);">
          <img src="./static/images/theorem-1.png" alt="Description of the image" style="max-width: 100%; height: auto; display: block;">
        </div>
        <!-- <p>
          The figure illustrates that PAL enables learning beyond a universal preference \(K^* > 1\) to identify diverse user preference groups. 
        </p> -->
        <p  style="text-align: justify; margin-top: 10px; max-width: 100%; margin-left: auto; margin-right: auto;">
          Observe that the bound decays as either the number of users \(N\) or the number of samples per user \(m\) increases. In addition, when \(N \ge \Omega(\dim x^2)\), the bound simplifies to \(\tilde{O} (\sqrt{K/m})\), which implies a per-user sample complexity of \(\tilde{O}(K)\). 
          This contrasts with the existing result of \(\tilde{O}(D)\) without mixture modeling (<a href="https://arxiv.org/abs/2207.03609">Canal et al., 2022</a>).
          The result captures the intuition that if the users amortize the cost of learning the common \(M\) and \(Q\), then each user only needs to individually learn their weights \(w_i \in \Delta^{K-1}\).
        </p>
      </div>
      <hr class="custom-line-dashed">
      <div class="column is-six-fifths" style="display:flex; align-items: center; flex-direction: column">
        <div style="display: inline-block; border: 2px solid blue; border-radius: 8px; text-align: center; margin: 20px; padding: 6px; width: 80%; box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);">
          <img src="./static/images/theorem-2.png" alt="Description of the image" style="max-width: 100%; height: auto; display: block;">
        </div>
        <!-- <p>
          The figure illustrates that PAL enables learning beyond a universal preference \(K^* > 1\) to identify diverse user preference groups. 
        </p> -->
        <p  style="text-align: justify; margin-top: 10px; max-width: 100%; margin-left: auto; margin-right: auto;">
          Intuitively, the first term captures how well the common mapping and the prototypes learned on seen users' dataset translate to new unseen users. 
          This term decays as the number of seen users \(N\) increases. 
          The second term characterizes how well our few-shot preference localization for a new unseen user generalizes to unseen pairs of this user. 
          This term indicates a sample complexity of \(\tilde{O}(K)\) and suggests efficient generalization, especially since \(K\) can be quite small in practice.
        </p>
      </div>
    </div>
    
    
    <hr class="custom-line">
  </div>
</section>









<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>@misc{chen2024palpluralisticalignmentframework,
        title={PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences}, 
        author={Daiwei Chen and Yi Chen and Aniket Rege and Ramya Korlakai Vinayak},
        year={2024},
        eprint={2406.08469},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2406.08469},
        }
      </code>
    </pre>
  </div>
</section>


<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
      <p>
        This work was supported by <b>NSF grants NCS-FO 2219903</b> and <b>NSF CAREER Award CCF 2238876</b>.
      </p>
      <p>
        <b>Usage and License Notice</b>: The data, code and model checkpoints are intended for research use.
      </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="font-size: 12px;">
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
